"""
Complete Baseline Experiment for VAD
Author: Kiro Chen (530337094)
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import numpy as np
import matplotlib.pyplot as plt

from src.feature_extractor import AudioFeatureExtractor
from src.vad_detector import VocalActivityDetector
from src.evaluation import VADEvaluator
from src.visualization import VADVisualizer

def create_synthetic_data(duration=30, sr=22050):
    """
    Create synthetic test audio with known vocal/instrumental segments

    Returns:
        audio: Audio signal
        ground_truth_frames: Frame-level labels
        ground_truth_segments: Segment-level labels
        times: Time stamps
    """
    print("Creating synthetic test data...")

    t = np.linspace(0, duration, int(sr * duration))
    audio = np.zeros_like(t)

    # Define vocal and instrumental sections
    vocal_sections = [(0, 5), (10, 15), (20, 25)]  # (start, end) in seconds

    for start, end in vocal_sections:
        mask = (t >= start) & (t < end)
        # Vocal: rich harmonics (fundamental + harmonics)
        audio[mask] = (
            np.sin(2 * np.pi * 440 * t[mask]) +  # A4
            0.5 * np.sin(2 * np.pi * 880 * t[mask]) +  # A5
            0.3 * np.sin(2 * np.pi * 1320 * t[mask]) +  # E6
            0.2 * np.sin(2 * np.pi * 1760 * t[mask])   # A6
        )

    # Fill instrumental sections with bass + drums simulation
    for i in range(int(duration)):
        if not any(start <= i < end for start, end in vocal_sections):
            mask = (t >= i) & (t < i + 1)
            # Instrumental: bass + simple rhythm
            audio[mask] += (
                0.7 * np.sin(2 * np.pi * 110 * t[mask]) +  # Bass
                0.3 * np.sin(2 * np.pi * 220 * t[mask]) +  # Bass harmonic
                0.2 * np.random.randn(np.sum(mask))  # Drums (noise)
            )

    # Add overall noise
    audio += 0.05 * np.random.randn(len(audio))

    # Normalize
    audio = audio / np.max(np.abs(audio))

    # Create ground truth labels
    hop_length = 512
    n_frames = int(np.ceil(len(audio) / hop_length))
    times = np.arange(n_frames) * hop_length / sr
    ground_truth_frames = np.zeros(n_frames)

    for start, end in vocal_sections:
        start_frame = int(start * sr / hop_length)
        end_frame = int(end * sr / hop_length)
        ground_truth_frames[start_frame:end_frame] = 1

    # Create ground truth segments
    ground_truth_segments = []
    in_vocal = False
    seg_start = 0

    for i, label in enumerate(ground_truth_frames):
        time = times[i]
        if label == 1 and not in_vocal:
            seg_start = time
            in_vocal = True
        elif label == 0 and in_vocal:
            ground_truth_segments.append(('vocal', seg_start, time))
            in_vocal = False

    if in_vocal:
        ground_truth_segments.append(('vocal', seg_start, times[-1]))

    # Add instrumental segments
    full_segments = []
    prev_end = 0
    for label, start, end in ground_truth_segments:
        if start > prev_end:
            full_segments.append(('instrumental', prev_end, start))
        full_segments.append((label, start, end))
        prev_end = end
    if prev_end < times[-1]:
        full_segments.append(('instrumental', prev_end, times[-1]))

    print(f"✓ Created {duration}s audio with {len(vocal_sections)} vocal sections")

    return audio, ground_truth_frames, full_segments, times

def run_baseline_experiments():
    """Run complete baseline experiments"""

    print("="*70)
    print("  ELEC5305 - VOCAL ACTIVITY DETECTION")
    print("  Baseline Experiments")
    print("  Student: Kiro Chen (530337094)")
    print("="*70)

    # Create synthetic data
    sr = 22050
    duration = 30
    audio, gt_frames, gt_segments, times = create_synthetic_data(duration=duration, sr=sr)

    # Initialize components
    evaluator = VADEvaluator()
    visualizer = VADVisualizer(sr=sr)

    # Test different baseline methods
    methods = ['energy', 'mfcc_variance', 'spectral']
    results = {}

    for method in methods:
        print(f"\n{'='*70}")
        print(f"Method: {method.upper()}")
        print('='*70)

        # Create detector
        detector = VocalActivityDetector(
            method=method,
            threshold=0.5,
            sr=sr,
            hop_length=512
        )

        # Detect
        predictions, pred_times, confidence = detector.detect(audio)

        # Ensure same length
        min_len = min(len(gt_frames), len(predictions))
        gt_frames_trimmed = gt_frames[:min_len]
        predictions_trimmed = predictions[:min_len]
        pred_times_trimmed = pred_times[:min_len]

        # Post-process
        predictions_smooth = detector.post_process(
            predictions_trimmed,
            min_duration_frames=5,
            median_size=5
        )

        # Get segments
        pred_segments = detector.get_segments(predictions_smooth, pred_times_trimmed)

        # Evaluate frame-level
        metrics_frame = evaluator.evaluate_frame_level(
            gt_frames_trimmed,
            predictions_smooth
        )

        # Evaluate segment-level
        metrics_segment = evaluator.evaluate_segment_level(
            gt_segments,
            pred_segments
        )

        # Print results
        evaluator.print_evaluation_report(metrics_frame, metrics_segment)

        # Store results
        results[method] = {
            'predictions': predictions_smooth,
            'times': pred_times_trimmed,
            'confidence': confidence[:min_len],
            'segments': pred_segments,
            'metrics_frame': metrics_frame,
            'metrics_segment': metrics_segment
        }

        # Visualize
        print(f"\nGenerating visualizations for {method}...")
        fig = visualizer.plot_detection_results(
            audio,
            predictions_smooth,
            pred_times_trimmed,
            ground_truth=gt_frames_trimmed,
            confidence=confidence[:min_len]
        )
        plt.savefig(f'results/vad_{method}_results.png', dpi=150, bbox_inches='tight')
        plt.close()

        fig_seg = visualizer.plot_segments(pred_segments, duration)
        plt.savefig(f'results/vad_{method}_segments.png', dpi=150, bbox_inches='tight')
        plt.close()

        print(f"✓ Saved visualizations to results/")

    # Compare methods
    print(f"\n{'='*70}")
    print("METHOD COMPARISON")
    print('='*70)

    print("\n{:<20s} {:<12s} {:<12s} {:<12s}".format(
        "Method", "Accuracy", "F1-Score", "Seg F1"
    ))
    print("-"*70)

    for method in methods:
        acc = results[method]['metrics_frame']['accuracy']
        f1 = results[method]['metrics_frame']['f1_score']
        seg_f1 = results[method]['metrics_segment']['segment_f1']
        print(f"{method:<20s} {acc:<12.4f} {f1:<12.4f} {seg_f1:<12.4f}")

    # Find best method
    best_method = max(results.keys(),
                     key=lambda m: results[m]['metrics_frame']['f1_score'])
    print(f"\n✓ Best performing method: {best_method.upper()}")

    print(f"\n{'='*70}")
    print("EXPERIMENT COMPLETED SUCCESSFULLY")
    print('='*70)

    return results

if __name__ == "__main__":
    # Create results directory
    os.makedirs('results', exist_ok=True)

    # Run experiments
    results = run_baseline_experiments()

    print("\n✓ All results saved to experiments/results/")
    print("✓ Check the generated PNG files for visualizations")